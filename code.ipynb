{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/paulbabu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/paulbabu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# External libraries for text similarity, NLP, date parsing, and sentiment analysis\n",
    "from thefuzz import fuzz, process  # thefuzz (formerly fuzzywuzzy) for fuzzy string matching\n",
    "import spacy                       # NLP library for Named Entity Recognition\n",
    "from datetime import datetime, timedelta  # Standard library for working with date and time\n",
    "import dateparser                  # Human-friendly natural language date parser\n",
    "import re                          # Regular expressions for text cleaning\n",
    "import nltk                        # Natural Language Toolkit for stopwords and lemmatization\n",
    "from nltk.corpus import stopwords  # Stopwords list\n",
    "from transformers import AutoTokenizer  # Tokenizer for model input formatting\n",
    "from collections import defaultdict      # Dictionary subclass for convenient default values\n",
    "from transformers import pipeline        # Prebuilt Hugging Face pipelines for NLP tasks\n",
    "import threading                   # Threading module for thread-safe operations\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    AI Model class for financial text analysis.\n",
    "\n",
    "    This class:\n",
    "    - Extracts organization names using Named Entity Recognition.\n",
    "    - Matches them against a provided Fortune 500 dictionary.\n",
    "    - Detects timeframes from plain text.\n",
    "    - Runs sentiment analysis using a FinBERT model.\n",
    "    - Updates prediction scores in a thread-safe manner.\n",
    "    \"\"\"\n",
    "\n",
    "    # Download NLTK resources (if not already present)\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "    def __init__(self, dict_fortune500, model=\"ProsusAI/finbert\", tokenizer=\"ProsusAI/finbert\"):\n",
    "        \"\"\"\n",
    "        Initializes the AI pipeline components.\n",
    "\n",
    "        Parameters:\n",
    "        - dict_fortune500 (dict): A dictionary mapping company names to their stock tickers.\n",
    "        - model (str): HuggingFace model name for sentiment analysis.\n",
    "        - tokenizer (str): HuggingFace tokenizer name.\n",
    "        \"\"\"\n",
    "        # Normalize all company names in the input dictionary for fuzzy matching\n",
    "        self.dict_fortune500 = {self.__normalize_text__(k): v for k, v in dict_fortune500.items()}\n",
    "\n",
    "        # Stores prediction scores for each matched organization\n",
    "        self.predictions = defaultdict(float)\n",
    "\n",
    "        # Load spaCy's small English model for NER\n",
    "        self.nlp_ner = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        # Load tokenizer for input preprocessing\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "\n",
    "        # Create a text-classification pipeline for sentiment prediction using FinBERT\n",
    "        self.pipe = pipeline(\"text-classification\", model=model)\n",
    "\n",
    "        # Lock to ensure thread-safe updates of prediction scores\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __normalize_text__(self, text):\n",
    "        \"\"\"\n",
    "        Normalize input text by lowercasing and stripping special characters.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): Input string to normalize.\n",
    "\n",
    "        Returns:\n",
    "        - str: Cleaned and normalized string.\n",
    "        \"\"\"\n",
    "        return ''.join(e for e in text.lower() if e.isalnum() or e.isspace()).strip()\n",
    "\n",
    "    def __match_organization__(self, extracted_org, threshold=80):\n",
    "        \"\"\"\n",
    "        Matches an extracted organization name to the closest Fortune 500 company name.\n",
    "\n",
    "        Parameters:\n",
    "        - extracted_org (str): The name detected by NER.\n",
    "        - threshold (int): Minimum fuzzy match score to consider a valid match.\n",
    "\n",
    "        Returns:\n",
    "        - str | None: The matching company's ticker symbol, or None if no match exceeds threshold.\n",
    "        \"\"\"\n",
    "        extracted_org_norm = self.__normalize_text__(extracted_org)\n",
    "\n",
    "        # Use fuzzy string matching to find the closest company name\n",
    "        best_match, score = process.extractOne(\n",
    "            extracted_org_norm,\n",
    "            list(self.dict_fortune500.keys()),\n",
    "            scorer=fuzz.token_sort_ratio\n",
    "        )\n",
    "\n",
    "        # Return ticker symbol if the match is strong enough\n",
    "        if score >= threshold:\n",
    "            return self.dict_fortune500[best_match]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __extract_entities__(self, text):\n",
    "        \"\"\"\n",
    "        Extracts organization names and estimates a time frame from the input text.\n",
    "\n",
    "        Uses:\n",
    "        - spaCy's NER model for organizations.\n",
    "        - dateparser to extract and classify any temporal reference.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The input text to analyze.\n",
    "\n",
    "        Returns:\n",
    "        - orgs (list): List of unique organization names detected.\n",
    "        - time_frame (str): One of ['Short-term', 'Medium-term', 'Long-term', 'Uncertain'].\n",
    "        \"\"\"\n",
    "        doc = self.nlp_ner(text)  # Apply NER model\n",
    "        orgs = set()              # Set to store unique organization names\n",
    "        time_frame = \"Uncertain\"  # Default classification\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"ORG\":\n",
    "                orgs.add(ent.text)\n",
    "\n",
    "        parsed_date = dateparser.parse(text)\n",
    "\n",
    "        if parsed_date:\n",
    "            current_date = datetime.today()\n",
    "\n",
    "            if parsed_date < current_date + timedelta(days=7):\n",
    "                time_frame = \"Short-term\"\n",
    "            elif parsed_date < current_date + timedelta(days=365):\n",
    "                time_frame = \"Medium-term\"\n",
    "            else:\n",
    "                time_frame = \"Long-term\"\n",
    "\n",
    "        return list(orgs), time_frame\n",
    "\n",
    "    def __preprocess_text__(self, text, max_tokens=512):\n",
    "        \"\"\"\n",
    "        Cleans and tokenizes the input text, preparing it for model input.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): Raw text to preprocess.\n",
    "        - max_tokens (int): Maximum number of tokens allowed by the model.\n",
    "\n",
    "        Returns:\n",
    "        - str: Preprocessed and truncated text.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        # Remove URLs and extra spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "        # Tokenize and truncate\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        tokens = tokens[:max_tokens]\n",
    "\n",
    "        return self.tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "    def __update_predictions__(self, orgs, result):\n",
    "        \"\"\"\n",
    "        Safely updates the prediction scores for the matched organizations.\n",
    "\n",
    "        Parameters:\n",
    "        - orgs (list): List of matched organization ticker symbols.\n",
    "        - result (dict): Sentiment prediction result from FinBERT.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            if result['label'] == 'positive':\n",
    "                for org in orgs:\n",
    "                    self.predictions[org] += result['score']\n",
    "            elif result['label'] == 'negative':\n",
    "                for org in orgs:\n",
    "                    self.predictions[org] -= result['score']\n",
    "\n",
    "    def predict(self, posts):\n",
    "        \"\"\"\n",
    "        Main prediction pipeline:\n",
    "        - Preprocess text.\n",
    "        - Extract organizations and time frame.\n",
    "        - Match organizations to Fortune 500 tickers.\n",
    "        - Predict sentiment using FinBERT.\n",
    "        - Update prediction scores.\n",
    "\n",
    "        Parameters:\n",
    "        - posts (str): Input text content.\n",
    "        \"\"\"\n",
    "        if len(posts) < 10:\n",
    "            return  # Skip very short texts\n",
    "\n",
    "        cleaned_text = self.__preprocess_text__(posts)\n",
    "        entities, timeframe = self.__extract_entities__(cleaned_text)\n",
    "\n",
    "        if len(entities) < 1:\n",
    "            return  # No organization detected\n",
    "\n",
    "        orgs = []\n",
    "        for i in entities:\n",
    "            matched_ticker = self.__match_organization__(i, 70)\n",
    "            if matched_ticker:\n",
    "                orgs.append(matched_ticker)\n",
    "\n",
    "        if len(orgs) > 0:\n",
    "            # Ensure text length is valid for the model\n",
    "            if \"Token indices sequence length is longer than the specified maximum sequence length for this model\" in cleaned_text:\n",
    "                return  # Invalid input for the model\n",
    "\n",
    "            results = self.pipe(cleaned_text[:512])\n",
    "            self.__update_predictions__(orgs, results[0])  # Thread-safe score update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "\n",
    "class Predictor:\n",
    "    \"\"\"\n",
    "    Predictor class to fetch financial-related Reddit posts and news articles,\n",
    "    and predict sentiment or relevance using a machine learning model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, companies={'Apple Inc.': 'AAPL'}, model=\"ProsusAI/finbert\", tokenizer=\"ProsusAI/finbert\"):\n",
    "        \"\"\"\n",
    "        Initialize the Predictor object.\n",
    "\n",
    "        Parameters:\n",
    "        - companies (dict): Mapping of company names to their stock tickers.\n",
    "        - model (str): Pretrained model identifier.\n",
    "        - tokenizer (str): Pretrained tokenizer identifier.\n",
    "        \"\"\"\n",
    "        self.companies = companies\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __create_Model__(self):\n",
    "        \"\"\"\n",
    "        Helper method to create a new instance of the prediction Model.\n",
    "\n",
    "        Returns:\n",
    "        - Model: An instance of the prediction model.\n",
    "        \"\"\"\n",
    "        return Model(self.companies, self.model, self.tokenizer)\n",
    "\n",
    "    def get_reddit_predictions(\n",
    "        self,\n",
    "        min_upvotes=5,\n",
    "        subreddits=['wallstreetbets', 'stocks', 'investing', 'options'],\n",
    "        start_date=None,\n",
    "        end_date=None,\n",
    "        CLIENT_ID=\"DrxLYbhcf1pudFwcNsV2Hw\",\n",
    "        CLIENT_SECRET=\"xVAig1oKsm1ET7NkNxiglGjpDFy6_g\",\n",
    "        USER_AGENT=\"my_reddit_stock_scraper v1.0 (by u/Funny_Low1871)\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fetch Reddit posts from specified subreddits within a date range,\n",
    "        filter by minimum upvotes, and run predictions on their text content.\n",
    "\n",
    "        Parameters:\n",
    "        - min_upvotes (int): Minimum number of upvotes a post must have to be considered.\n",
    "        - subreddits (list): List of subreddit names to search.\n",
    "        - start_date (str): Start date in 'YYYY-MM-DD' format (default is yesterday).\n",
    "        - end_date (str): End date in 'YYYY-MM-DD' format (default is now).\n",
    "        - CLIENT_ID (str): Reddit API client ID (required).\n",
    "        - CLIENT_SECRET (str): Reddit API client secret (required).\n",
    "        - USER_AGENT (str): Reddit API user agent (required).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Predictions made by the model.\n",
    "        \"\"\"\n",
    "        redditModel = self.__create_Model__()\n",
    "\n",
    "        # Handle default start and end dates\n",
    "        if start_date is None:\n",
    "            start_dt = datetime.utcnow().date() - timedelta(days=1)\n",
    "            start_dt = datetime.combine(start_dt, datetime.min.time())\n",
    "        else:\n",
    "            start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "\n",
    "        if end_date is None:\n",
    "            end_dt = datetime.utcnow()\n",
    "        else:\n",
    "            end_dt = datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1)\n",
    "\n",
    "        start_time = int(start_dt.timestamp())\n",
    "        end_time = int(end_dt.timestamp())\n",
    "\n",
    "        # Reddit API credentials (replace with your own if needed)\n",
    "        CLIENT_ID = CLIENT_ID\n",
    "        CLIENT_SECRET = CLIENT_SECRET\n",
    "        USER_AGENT = USER_AGENT\n",
    "\n",
    "        # Initialize Reddit API client using PRAW\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=CLIENT_ID,\n",
    "            client_secret=CLIENT_SECRET,\n",
    "            user_agent=USER_AGENT\n",
    "        )\n",
    "\n",
    "        def process_subreddit(name):\n",
    "            \"\"\"\n",
    "            Fetches and processes posts from a single subreddit.\n",
    "\n",
    "            Parameters:\n",
    "            - name (str): Name of the subreddit.\n",
    "            \"\"\"\n",
    "            count = 0\n",
    "            subreddit = reddit.subreddit(name)\n",
    "\n",
    "            for post in subreddit.new(limit=1000):\n",
    "                post_time = int(post.created_utc)\n",
    "\n",
    "                if post_time < start_time:\n",
    "                    break  # Posts older than start_time are skipped\n",
    "\n",
    "                count += 1\n",
    "\n",
    "                # Apply date and upvote filter\n",
    "                if start_time <= post_time <= end_time and post.score >= min_upvotes:\n",
    "                    post_data = {\"text\": f\"{post.title} {post.selftext}\"}\n",
    "                    redditModel.predict(post_data[\"text\"])  # Predict sentiment or relevance\n",
    "\n",
    "            print(f\"Processed {count} posts from subreddit: {name}\")\n",
    "\n",
    "        # Multi-threaded processing of subreddits\n",
    "        with ThreadPoolExecutor(max_workers=len(subreddits)) as executor:\n",
    "            future_to_subreddit = {executor.submit(process_subreddit, name): name for name in subreddits}\n",
    "\n",
    "        for future in as_completed(future_to_subreddit):\n",
    "            subreddit = future_to_subreddit[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"A thread raised an exception in subreddit '{subreddit}': {e}\")\n",
    "\n",
    "        return redditModel.predictions\n",
    "\n",
    "    def predict_from_news(\n",
    "            self,\n",
    "            api_key=\"80a4b4a81d11446d8640c1d38ccb7051\",\n",
    "            query=\"stocks\",\n",
    "            from_date=None,\n",
    "            to_date=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fetch news articles using NewsAPI and predict sentiment or relevance.\n",
    "\n",
    "        Parameters:\n",
    "        - api_key (str): Your NewsAPI authentication key.\n",
    "        - query (str): Search keyword for news articles.\n",
    "        - from_date (str): Start date in 'YYYY-MM-DD' format (default: 24 hours ago).\n",
    "        - to_date (str): End date in 'YYYY-MM-DD' format (default: today).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Predictions made by the model.\n",
    "        \"\"\"\n",
    "        newsModel = self.__create_Model__()\n",
    "\n",
    "        # Set default date range: last 24 hours\n",
    "        if to_date is None:\n",
    "            to_date = datetime.utcnow().date().isoformat()\n",
    "\n",
    "        if from_date is None:\n",
    "            from_date = (datetime.utcnow() - timedelta(hours=24)).date().isoformat()\n",
    "\n",
    "        url = \"https://newsapi.org/v2/everything\"\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'from': from_date,\n",
    "            'to': to_date,\n",
    "            'sortBy': 'publishedAt',\n",
    "            'language': 'en',\n",
    "            'apiKey': api_key\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            articles = response.json().get(\"articles\", [])\n",
    "            if not articles:\n",
    "                print(f\"No news articles found for query '{query}' between {from_date} and {to_date}.\")\n",
    "            else:\n",
    "                for i, article in enumerate(articles, 1):\n",
    "                    newsModel.predict(article[\"title\"])  # Predict sentiment or relevance\n",
    "        else:\n",
    "            print(f\"Error {response.status_code}: {response.json().get('message')}\")\n",
    "\n",
    "        return newsModel.predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class StockEvaluator:\n",
    "    \n",
    "    def fetch_stock_data(self, ticker: str, date: str):\n",
    "        \"\"\"\n",
    "        Fetches the stock data (open, close, and price change) for a given stock ticker on a specific date.\n",
    "\n",
    "        Args:\n",
    "            ticker (str): Stock ticker symbol (e.g., 'AAPL', 'TSLA').\n",
    "            date (str): Date in 'YYYY-MM-DD' format.\n",
    "\n",
    "        Returns:\n",
    "            dict: Contains open price, close price, and the change in price.\n",
    "                  Returns an error message if no data is found.\n",
    "        \"\"\"\n",
    "        # Create a Ticker object using the yfinance API\n",
    "        stock = yf.Ticker(ticker)\n",
    "        \n",
    "        # Prepare the date range to query the Yahoo Finance API\n",
    "        start_date = date\n",
    "        end_date = (datetime.strptime(date, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        # Fetch historical stock data from Yahoo Finance\n",
    "        data = stock.history(start=start_date, end=end_date)\n",
    "\n",
    "        # Check if no data was found (e.g., market may be closed)\n",
    "        if data.empty:\n",
    "            return {\"error\": f\"No data found for {ticker} on {date} (market may have been closed).\"}\n",
    "\n",
    "        # Extract open, close, and price change values\n",
    "        open_price = data['Open'].iloc[0]\n",
    "        close_price = data['Close'].iloc[0]\n",
    "        price_change = close_price - open_price\n",
    "\n",
    "        # Return the stock data\n",
    "        return {\n",
    "            \"ticker\": ticker,\n",
    "            \"date\": date,\n",
    "            \"open\": round(open_price, 2),\n",
    "            \"close\": round(close_price, 2),\n",
    "            \"change\": round(price_change, 2)\n",
    "        }\n",
    "    \n",
    "    def evaluate_predictions(self, predictions: dict, date: str):\n",
    "        \"\"\"\n",
    "        Evaluates stock predictions by fetching actual stock data for the given date \n",
    "        and compares it to the predicted sentiment scores.\n",
    "\n",
    "        Args:\n",
    "            predictions (dict): A dictionary where keys are stock tickers and values are predicted scores.\n",
    "            date (str): The date in 'YYYY-MM-DD' format.\n",
    "\n",
    "        Prints the actual stock data and compares it to the predicted score.\n",
    "        \"\"\"\n",
    "        for ticker, prediction in predictions.items():\n",
    "            # Get actual stock data for the ticker on the specified date\n",
    "            result = self.fetch_stock_data(ticker, date)\n",
    "            \n",
    "            # Print predicted score and the actual stock data\n",
    "            print(f\"\\nTicker: {ticker}\")\n",
    "            print(f\"Predicted Score: {prediction}\")\n",
    "            print(f\"Stock Data: {result}\")\n",
    "    \n",
    "    def evaluate_sentiment_accuracy(self, predictions: dict, date: str):\n",
    "        \"\"\"\n",
    "        Evaluates the sentiment classification predictions against actual stock price movements \n",
    "        and calculates accuracy, precision, recall, and F1 score.\n",
    "\n",
    "        Args:\n",
    "            predictions (dict): A dictionary where keys are stock tickers and values are predicted sentiment scores.\n",
    "            date (str): The date in 'YYYY-MM-DD' format.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing accuracy, precision, recall, and F1 score.\n",
    "        \"\"\"\n",
    "        # Initialize counters for the confusion matrix\n",
    "        TP = TN = FP = FN = 0\n",
    "\n",
    "        # Iterate through the predictions and calculate the confusion matrix\n",
    "        for ticker, prediction in predictions.items():\n",
    "            result = self.fetch_stock_data(ticker, date)\n",
    "            \n",
    "            # Ensure the result contains valid price change data\n",
    "            if result and 'change' in result:\n",
    "                predicted_up = prediction >= 0  # Predicted sentiment (up or down)\n",
    "                actual_up = result['change'] >= 0  # Actual stock movement (up or down)\n",
    "\n",
    "                # Update the confusion matrix\n",
    "                if predicted_up and actual_up:\n",
    "                    TP += 1  # True positive\n",
    "                elif predicted_up and not actual_up:\n",
    "                    FP += 1  # False positive\n",
    "                elif not predicted_up and actual_up:\n",
    "                    FN += 1  # False negative\n",
    "                else:\n",
    "                    TN += 1  # True negative\n",
    "\n",
    "                # Print prediction result for each ticker\n",
    "                status = \"✅ Correct\" if predicted_up == actual_up else \"❌ Wrong\"\n",
    "                print(f\"{ticker}: Predicted={prediction:.2f}, Actual Change={result['change']:.2f} --> {status}\")\n",
    "\n",
    "        # Calculate performance metrics\n",
    "        total = TP + TN + FP + FN\n",
    "        if total == 0:\n",
    "            print(\"\\nNo valid stock data to evaluate.\")\n",
    "            return\n",
    "\n",
    "        accuracy = (TP + TN) / total\n",
    "        precision = TP / (TP + FP) if TP + FP else 0\n",
    "        recall = TP / (TP + FN) if TP + FN else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if precision + recall else 0\n",
    "\n",
    "        # Return the evaluation metrics\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1_score\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Plotter:\n",
    "    def __init__(self):\n",
    "        self.dates = []\n",
    "        self.predictions = []\n",
    "        self.actual_changes = []\n",
    "\n",
    "    def update_data(self, date: str, prediction: float, actual_change: float):\n",
    "        self.dates.append(date)\n",
    "        self.predictions.append(prediction)\n",
    "        self.actual_changes.append(actual_change)\n",
    "\n",
    "    def plot_comparison(self):\n",
    "        \"\"\"\n",
    "        Plots a comparison between the predicted stock sentiment and actual stock price changes.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.dates, self.predictions, label='Predicted Sentiment', marker='o')\n",
    "        plt.plot(self.dates, self.actual_changes, label='Actual Stock Change', marker='x')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Sentiment / Stock Change')\n",
    "        plt.title('Predicted Sentiment vs Actual Stock Change')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today: 2025-04-15, Yesterday: 2025-04-14, Two days ago: 2025-04-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 posts from subreddit: ETF\n",
      "Processed 2 posts from subreddit: SPACs\n",
      "Processed 0 posts from subreddit: StockNews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2 posts from subreddit: securityanalysis\n",
      "Processed 6 posts from subreddit: RobinHood\n",
      "Processed 66 posts from subreddit: Forex\n",
      "Processed 6 posts from subreddit: FinancialIndependence\n",
      "Processed 38 posts from subreddit: quantfinance\n",
      "Processed 15 posts from subreddit: algotrading\n",
      "Processed 107 posts from subreddit: dividends\n",
      "Processed 45 posts from subreddit: pennystocks\n",
      "Processed 51 posts from subreddit: options\n",
      "Processed 49 posts from subreddit: ValueInvesting\n",
      "Processed 78 posts from subreddit: wallstreetbets\n",
      "Processed 235 posts from subreddit: Daytrading\n",
      "Processed 106 posts from subreddit: investing\n",
      "Processed 102 posts from subreddit: StockMarket\n",
      "Processed 489 posts from subreddit: personalfinance\n",
      "Processed 138 posts from subreddit: stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 posts from subreddit: ETF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 posts from subreddit: StockNews\n",
      "Processed 1 posts from subreddit: securityanalysis\n",
      "Processed 6 posts from subreddit: RobinHood\n",
      "Processed 2 posts from subreddit: SPACs\n",
      "Processed 2 posts from subreddit: FinancialIndependence\n",
      "Processed 47 posts from subreddit: Forex\n",
      "Processed 10 posts from subreddit: algotrading\n",
      "Processed 77 posts from subreddit: dividends\n",
      "Processed 30 posts from subreddit: quantfinance\n",
      "Processed 37 posts from subreddit: pennystocks\n",
      "Processed 31 posts from subreddit: ValueInvesting\n",
      "Processed 69 posts from subreddit: investing\n",
      "Processed 169 posts from subreddit: Daytrading\n",
      "Processed 43 posts from subreddit: options\n",
      "Processed 62 posts from subreddit: StockMarket\n",
      "Processed 315 posts from subreddit: personalfinance\n",
      "Processed 59 posts from subreddit: wallstreetbets\n",
      "Processed 98 posts from subreddit: stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VST: Predicted=0.84, Actual Change=2.49 --> ✅ Correct\n",
      "FDX: Predicted=-0.88, Actual Change=-2.59 --> ✅ Correct\n",
      "CCI: Predicted=-0.90, Actual Change=0.53 --> ❌ Wrong\n",
      "TECH: Predicted=0.88, Actual Change=-1.19 --> ❌ Wrong\n",
      "NVDA: Predicted=0.89, Actual Change=1.23 --> ✅ Correct\n",
      "NDAQ: Predicted=0.05, Actual Change=0.22 --> ✅ Correct\n",
      "WFC: Predicted=0.77, Actual Change=1.10 --> ✅ Correct\n",
      "ON: Predicted=0.77, Actual Change=0.10 --> ✅ Correct\n",
      "UAL: Predicted=0.77, Actual Change=1.35 --> ✅ Correct\n",
      "MTB: Predicted=0.77, Actual Change=0.94 --> ✅ Correct\n",
      "MS: Predicted=0.77, Actual Change=0.35 --> ✅ Correct\n",
      "JNJ: Predicted=0.77, Actual Change=-2.08 --> ❌ Wrong\n",
      "NFLX: Predicted=0.77, Actual Change=26.28 --> ✅ Correct\n",
      "BAC: Predicted=0.77, Actual Change=0.19 --> ✅ Correct\n",
      "BX: Predicted=-0.82, Actual Change=3.11 --> ❌ Wrong\n",
      "HII: Predicted=-0.66, Actual Change=-0.68 --> ✅ Correct\n",
      "AMT: Predicted=-1.81, Actual Change=-1.27 --> ✅ Correct\n",
      "AAPL: Predicted=0.90, Actual Change=0.29 --> ✅ Correct\n",
      "TKO: Predicted=-0.67, Actual Change=0.81 --> ❌ Wrong\n",
      "AAPL: Predicted=0.59, Actual Change=0.29 --> ✅ Correct\n",
      "MS: Predicted=0.79, Actual Change=0.35 --> ✅ Correct\n",
      "IP: Predicted=0.74, Actual Change=-0.71 --> ❌ Wrong\n",
      "\n",
      "Reddit Predictions Results:\n",
      "{'accuracy': 0.7777777777777778, 'precision': 0.8461538461538461, 'recall': 0.8461538461538461, 'f1_score': 0.8461538461538461}\n",
      "\n",
      "News Predictions Results:\n",
      "{'accuracy': 0.5, 'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1_score': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def main():\n",
    "    # Load the Fortune 500 data\n",
    "    fortune_500_companies = pd.read_csv('fortune_500_list.csv')\n",
    "    fortune_500_companies[\"Company Name\"] = fortune_500_companies[\"Security\"]\n",
    "    dict_fortune500 = dict(zip(fortune_500_companies['Company Name'], fortune_500_companies['Symbol']))\n",
    "    subreddits = [\n",
    "    'wallstreetbets', 'stocks', 'investing', 'options',\n",
    "    'StockMarket', 'pennystocks', 'securityanalysis', 'ValueInvesting',\n",
    "    'dividends', 'RobinHood', 'Daytrading', 'Forex', 'quantfinance',\n",
    "    'FinancialIndependence', 'SPACs', 'personalfinance', 'ETF',\n",
    "    'algotrading', 'StockNews'\n",
    "]\n",
    "    # Get the last two days\n",
    "    today = datetime.today().date()\n",
    "    yesterday = str(today - timedelta(days=1))\n",
    "    two_days_ago = str(today - timedelta(days=2))\n",
    "    today= str(today)\n",
    "    print(f\"Today: {today}, Yesterday: {yesterday}, Two days ago: {two_days_ago}\")\n",
    "    # Set up the Evaluator and Plotter\n",
    "    evaluator = StockEvaluator()\n",
    "    plotter = Plotter()\n",
    "    # Get Reddit predictions    \n",
    "    predictor= Predictor(companies=dict_fortune500)\n",
    "\n",
    "    \n",
    "    reddit_predictions1 = predictor.get_reddit_predictions(min_upvotes=5,subreddits=subreddits, start_date=two_days_ago, end_date=yesterday)\n",
    "    news_predictions1 = predictor.predict_from_news(from_date=two_days_ago, to_date=yesterday)\n",
    "\n",
    "    # reddit_predictions2 = predictor.get_reddit_predictions(min_upvotes=5,subreddits=subreddits, start_date=yesterday, end_date=today)\n",
    "    # news_predictions2 = predictor.predict_from_news(from_date=yesterday, to_date=today)\n",
    "    \n",
    "    results_reddit = evaluator.evaluate_sentiment_accuracy(reddit_predictions1, today)\n",
    "    results_news = evaluator.evaluate_sentiment_accuracy(news_predictions1, today)\n",
    "\n",
    "    # Print the results     \n",
    "    print(\"\\nReddit Predictions Results:\")\n",
    "    print(results_reddit)\n",
    "    print(\"\\nNews Predictions Results:\")\n",
    "    print(results_news)\n",
    "main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
